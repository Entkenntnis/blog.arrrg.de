---
title:  "Reinforcement Learning - Phần 1"
mathjax: true
layout: post
categories: media
---


### 1. Giới thiệu về reinforcement learning 

Reinforcement learning là một nhánh đặc biệt của học máy và **không thuộc cả 2 loại supervised learning và unsupervised learning**.  Lý do nó không phải là supervised learning và unsupervised learning là vì phương pháp này không cần đến labels và không được thiết kế để tìm ra cấu trúc ẩn (hidden structure) của dữ liệu. Đây là một giải thuật học bằng trials-and-errors. Trong RL, chúng ta sẽ huấn luyện agent học cách hành động trong một môi trường (environment), ta chỉ cần đặt mục tiêu và agent sẽ **tìm ra phương án để đạt được mục tiêu đó**.  

<figure style="text-align: center">
<img src="https://lilianweng.github.io/posts/2018-02-19-rl-overview/RL_illustration.png" alt="">
</figure>


Ví dụ, trong cờ tướng chúng ta biết thế nào là thắng nhưng còn **làm sao để thắng** thì vẫn là ẩn số (LoL). Tương tự với Cờ vua, quay rubic, chứng khoán, Liên Minh Huyền Thoại, Fifa, đột kích, ..., chúng ta đều biết như thế nào là thắng hoặc hoàn thành nhiệm vụ, tuy nhiên con đường đến đó chúng ta chưa biết. Và giải thuật reinforcement learning sẽ giúp tìm ra lời giải cho bài toán **làm sao** này.  


### 2. Lịch sử của reinforcement learning 

Reinforcement Learning (RL) bắt nguồn từ lý thuyết hành vi trong tâm lý học, với khái niệm thử và sai và phần thưởng từ thập niên 1950. Những nhà tiên phong như Richard Bellman đã phát triển các khái niệm nền tảng như Dynamic Programming và Bellman Equation, đặt nền móng cho lý thuyết RL. Trong những năm 1980, Watkins giới thiệu Q-learning, một phương pháp nổi tiếng giúp tác nhân học cách tối ưu hóa hành động mà không cần biết trước mô hình môi trường.

Tuy nhiên, RL chỉ thực sự bùng nổ vào đầu thập niên 2010, khi DeepMind phát triển Deep Q Networks (DQN), kết hợp RL với mạng CNN, cho phép agent học qua hình ảnh và đánh bại con người trong trò chơi Atari. Sau đó, thành công của các hệ thống như AlphaGo, OpenAI Five, ChatGPT, và nhiều ứng dụng trong tự động hóa đã đưa RL lên tầm cao mới, trở thành lĩnh vực nổi bật trong AI.

<figure style="text-align: center">
<img src="https://s.yimg.com/ny/api/res/1.2/PWLMLHYqmFgzGn_R9VdIzA--/YXBwaWQ9aGlnaGxhbmRlcjt3PTk2MDtoPTU0MDtjZj13ZWJw/https://o.aolcdn.com/hss/storage/midas/64f5c0fa4cd3cdaee7906927a26f3b29/203536496/youtu.be-qUAmTYHEyM8+%287%29-1400.jpg">
<figcaption><b>Hình 1.2.</b> AlphaGo đánh bại kỳ thủ cờ vây số 1 thế giới Lee Sedol </figcaption>
</figure>

### 3. Khái niệm và thuật ngữ trong RL

Hai khái niệm chính trong RL là agent và environment. Environment là nơi mà agent sẽ tương tác. Giả sử trong Liên Minh thì summoner rift là môi trường còn tướng là agent. Ở mỗi một thời điểm nào đó, agent sẽ dựa vào quan sát để đưa ra một hoặc nhiều quyết định hành động và điều này dựa vào quan sát của nó vào môi trường. Ví dụ khi agent thấy bị chiếu tướng thì agent sẽ đưa ra quyết định dời tướng đi qua địa điểm khác để tránh bị chiếu tướng. Và khi agent hành động thì môi trường sẽ thay đổi sang trạng thái mới. 

Với mỗi quyết định của agent trong mỗi tình huống, nó sẽ nhận được phần thưởng (**reward**). Ví dụ như phần thưởng là điểm bài kiểm tra thì quyết định học bài của agent sẽ cho reward cao và ngược lại. Nhiệm vụ quan trọng nhất của agent là làm sao để đạt được tích lũy phần thưởng lớn nhất (maximum cumulative reward), hay còn được gọi là **return**. 

Tiếp theo, mình sẽ giới thiệu về các khái niệm chuyên sâu hơn của RL: 

* **States and observations**

Một trạng thái (state) s là mô tả hoàn chỉnh về trạng thái của thế giới. Không có thông tin nào về thế giới bị ẩn khỏi trạng thái. Một quan sát (observation) o là mô tả một phần của trạng thái, có thể bỏ qua một số thông tin.

Trong học tăng cường sâu (deep RL), chúng ta hầu như luôn biểu diễn các trạng thái và quan sát bằng một vector giá trị thực, ma trận hoặc tensor bậc cao hơn. Ví dụ, một quan sát hình ảnh có thể được biểu diễn bằng ma trận RGB của các giá trị pixel; trạng thái của một robot có thể được biểu diễn bởi các góc khớp và vận tốc của nó.

Khi tác nhân có thể quan sát toàn bộ trạng thái của môi trường, chúng ta nói rằng môi trường được quan sát đầy đủ (fully observed). Khi tác nhân chỉ có thể thấy một phần quan sát, chúng ta nói rằng môi trường được quan sát một phần (partially observed).

* **Action spaces**

Các môi trường khác nhau cho phép các loại hành động khác nhau. Tập hợp tất cả các hành động hợp lệ trong một môi trường nhất định thường được gọi là không gian hành động (action space). Một số môi trường, như Atari và Go, có không gian hành động rời rạc (discrete action space), nơi chỉ có một số lượng hữu hạn các nước đi có sẵn cho tác nhân. Các môi trường khác, như khi tác nhân điều khiển robot trong thế giới vật lý, có không gian hành động liên tục (continuous action space). Trong các không gian liên tục, các hành động là các vector giá trị thực.

Sự phân biệt này có những hệ quả khá sâu sắc đối với các phương pháp trong học tăng cường sâu (deep RL). Một số họ thuật toán chỉ có thể được áp dụng trực tiếp trong một trường hợp, và sẽ phải được sửa đổi đáng kể để áp dụng cho trường hợp còn lại.

* **Policies**

A policy is a rule used by an agent to decide what actions to take. It can be deterministic, in which case it is usually denoted by $\mu$:

$$a_t = \mu(s_t)$$

or it may be stochastic, in which case it is usually denoted by $\pi$:

$$a_t \sim \pi(\cdot|s_t)$$

Bởi vì policy như là não của agent nên sẽ có nhiều blog và paper sử dụng agent và policy với ý nghĩa như nhau. 

Với deep RL, các policy được parameterised nên chúng sẽ có notation như sau:

$$a_t = \mu_\theta(s_t)$$

$$a_t \sim \pi_\theta(\cdot | s_t)$$

Trong policy, ta có 2 loại là deterministic và stochastic. Trong đó deterministic là loại đưa ra quyết định cứng trong khi stochastic đưa ra các quyết định ngẫu nhiên dựa vào một phân phối. 

```python
# Deterministic
pi_net = nn.Sequential(
              nn.Linear(obs_dim, 64),
              nn.Tanh(),
              nn.Linear(64, 64),
              nn.Tanh(),
              nn.Linear(64, act_dim)
            )
```

```python
# Stochastic

```

* **Trajectories**

Trajectory $\tau$ là một chuỗi các states và actions của agent trong environment. 

$$\tau = (s_0, a_0, s_1, a_1, ...)$$

State 0 là trạng thái ban đầu của agent trong environment, thường được kí hiệu bằng $\rho_0$:

$$s_0 \sim \rho_0(\cdot).$$

Ở mỗi state bất kì, agent sẽ chọn một hành động và sau khi thực hiện hành động này thì môi trường sẽ chuyển sang một state mới và quá trình này được gọi là _state transition_. State transition được quyết định bởi môi trường, nó có thể được biểu diễn bằng hàm deterministic, 

$$s_{t+1} = f(s_t, a_t)$$

hoặc stochastic, 

$$s_{t+1} \sim P(\cdot|s_t, a_t)$$

* **Returns**



### 4. Tasks và giải thuật trong Reinforcement learning