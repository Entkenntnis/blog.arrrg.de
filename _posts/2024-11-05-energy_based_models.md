---
title:  "Energy-based models"
mathjax: true
layout: post
categories: media
---



<figure style="text-align: center">
<img src="https://static.toiimg.com/thumb/msid-114044502,width-1280,height-720,imgsize-1299966,resizemode-6,overlay-toi_sw,pt-32,y_pad-40/photo.jpg" alt="">
</figure>

### 1. Introduction to Energy-based models (EBMs)

Energy-based models (EBMs) are a class of probabilistic models that assign an energy to each configuration of the input data and learn to identify low-energy configurations. Unlike traditional supervised learning methods that directly predict output from input, EBMs instead use an energy function to determine stable configurations. 

* **Key Concepts:**

* * **Energy function**:  An energy function is a scalar function that maps input configurations to energy values. The goal is to find low-energy configurations, which correspond to stable states or patterns in the data.

* * **State:** Each possible configuration of the model (e.g., the activations in a neural network) represents a state.

* * **Energy Minimisation:** EBMs learn by finding the lowest-energy states, effectively discovering the most probable or stable states.

* **Why EBMs are relevant:**

* * They offer a flexible approach to model complex distributions without requiring explicit probability calculations. 

* * EBMs have foundational importance in unsupervised learning and associative memory and have influenced many modern architectures. 

### 2. Core concepts of energy-based models

### 3. Restricted Boltzmann Machines (RBMs)

### 4. Hopfield Networks

### 5. Modern extensions and relevance

### 6. Conclusion