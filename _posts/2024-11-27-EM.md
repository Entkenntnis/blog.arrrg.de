---
title:  "Expectation-Maximisation Explained"
mathjax: true
layout: post
categories: media
---


### 1. Problem statement

Edmond is an intellectually outstanding student. One day, he is tasked with measuring heights of all the students in his school. The task requires filling in 3 fields: name, gender, and height. Unfortunately, he forgets to record the gender of some students and worries that the teacher might notice and think poorly of him. Being a smart and trustworthy student, Edmond cannot let that happen. Determined to fix his mistake, he searches for an efficient and accurate way to fill in the missing information. That is how he comes across the concept of "Expectation-Maximisation". 

### 2. Expectation-Maximisation (EM)

<figure style="text-align: center">
<img src="https://upload.wikimedia.org/wikipedia/commons/6/69/EM_Clustering_of_Old_Faithful_data.gif" alt="">
</figure>

In general, the Expectation-Maximisation (EM) algorithm is used to estimate hidden variables or distributions from observed data when some information is incomplete or missing. It iteratively alternates between assigning probabilities to the missing data (Expectation step) and optimizing parameters based on these assignments (Maximisation step).

In Edmond's case, the observed data consists of the names and heights of the students, while the missing data is the gender. By applying EM, Edmond can i**nfer the gender** for each recorded height based on statistical patterns, such as the distribution of heights typically associated with different genders in the school population. This allows him to fill in the missing fields accurately, even though the gender information was initially incomplete.

### 3. How it works

<figure style="text-align: center">
<img src="https://media.geeksforgeeks.org/wp-content/uploads/20190512202126/em11.jpg" alt="">
</figure>

Given the *statistical model* which generates a set $\mathbf{X}$ of observed data, a set of unobserved latent data or *missing values* $\mathbf{Z}$, and a vector of unknown parameters $\theta$, along with a *likelihood function* $L(\theta; \mathbf{X}, \mathbf{Z}) = p(\mathbf{X}, \mathbf{Z} \mid \theta)$ the *maximum likelihood estimate* (MLE) of the unknown parameters is determined by maximizing the *marginal likelihood* of the observed data:

$$ 
L(\theta; \mathbf{X}) = p(\mathbf{X} \mid \theta) = \int p(\mathbf{X}, \mathbf{Z} \mid \theta) p(\mathbf{Z} \mid \theta) \, d\mathbf{Z}.
$$

However, this quantity is often intractable since $\mathbf{Z}$ is unobserved and the distribution of $\mathbf{Z}$ is unknown before attaining $\theta$.

### The EM Algorithm

The EM algorithm seeks to find the maximum likelihood estimate of the marginal likelihood by iteratively applying these two steps:

**Expectation step (E step):** Define $Q(\theta \mid \theta^{(t)})$ as the *expected value* of the log *likelihood function* of $\theta$, with respect to the current *conditional distribution* of $\mathbf{Z}$ given $\mathbf{X}$ and the current estimates of the parameters $\theta^{(t)}$:

$$
Q(\theta \mid \theta^{(t)}) = \mathbb{E}_{Z \sim p(\cdot \mid \mathbf{X}, \theta^{(t)})} \left[ \log p(\mathbf{X}, \mathbf{Z} \mid \theta) \right].
$$

**Maximization step (M step):** Find the parameters that maximize this quantity:

$$
\theta^{(t+1)} = \underset{\theta}{\operatorname{arg\,max}} \, Q(\theta \mid \theta^{(t)}).
$$

More succinctly, we can write it as one equation:

$$
\theta^{(t+1)} = \underset{\theta}{\operatorname{arg\,max}} \, \mathbb{E}_{Z \sim p(\cdot \mid \mathbf{X}, \theta^{(t)})} \left[ \log p(\mathbf{X}, \mathbf{Z} \mid \theta) \right].
$$


### 4. Hands-on example



### 5. Conclusion