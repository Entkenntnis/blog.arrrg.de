---
title:  "Information Theory Applications Glimpse in Statistics"
mathjax: true
author: Mehrdad Mohammadi
category: Information Theoey and Learning Theory 
---
**Era of massive data sets brings fascinating problems at the interfaces between information theory and
(statistical) learning theory.**
<br/>
There are various studied connections between information theory and statistics:
<ul>
  <li>Hypothesis testing, large deviations</li>
  <li>Fisher information, Kullback-Leibler divergence</li>
  <li>Metric entropy and Fanoâ€™s inequality</li>
  <il> etc...</li>
</ul>
 In their classic paper, Kolmogorov and Tikhomirov(1959) make connections between statistical estimation, metric entropy and the notion of channel capacity. Let's write and draw this in information theoretic jargon. Let;
  
<ul>
  <li>**Codebook:** indexed family of probability distributions</li>
  <li>**Codeword:** nature chooses some</li>
  <li>**Channel:** user observes n i.i.d. draws</li>
  <il> **Decoding:** estimator </li>
</ul>
