---
title: "Giải thích và code WGAN"
mathjax: true
layout: post
---

Xin chào các bạn, 
Trong bài post này, mình sẽ giải thích về  các công thức toán học Wasserstein GAN (WGAN) và code kiến trúc này from scratch với Pytorch. 


### 1. Giới thiệu 

WGAN là một kiến trúc tạo sinh (generative model) giống như GAN truyền thống với một vài sự thay đổi để  cải thiện performance cho kiến trúc GAN truyền thống. Như các bạn đã biết, việc huấn luyện GAN truyền thống sẽ gặp rất nhiều khó khăn như collapse mode, non-convergence, diminished gradient, unbalance training, sensitive to hyperparameters, ... Các bạn có thể đọc thêm bài viết của Jonathan Hui để biết thêm chi tiết về các nhược điểm của GANs truyền thống, trong post của tác giả có liệt kê đầy đủ và giải thích chi tiết các nhược điểm cố hữu của GANs truyền thống. 


### 2. Ưu điểm của WGAN so với GANs truyền thống 



### 3. Code WGAN with Pytorch 

Về cơ bản, cách xây dựng generator và discriminator của WGAN giống với GAN truyền thống. 

* **Step 1: Import các thư viện cần thiết**

```python
import torch
from torch import nn, optim
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader, Dataset
from torchvision.datasets import MNIST
import torchvision
import torch.nn.functional as F
from tqdm import tqdm
import torchvision.utils as vutils

%matplotlib inline
```

* **Step 2: Load dataset**

```python
transform = torchvision.transforms.Compose([
    torchvision.transforms.ToTensor()
                                           ])

BATCH_SIZE = 64
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

train_loader = DataLoader(MNIST("./data/", train=True, download=True, transform=transform), batch_size = BATCH_SIZE, shuffle=True)
```

* **Step 3: Xây dựng mô hình Generator và Discriminator**

Đầu tiên, ta sẽ xây dựng khối convolution gồm Conv + Batch Norm + ReLU
```python
class Conv(nn.Module):
    def __init__(self, in_channels: int, out_channels: int,
                 kernel_size: int = 3, stride: int = 1, padding: int =1):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)
        self.bn = nn.BatchNorm2d(out_channels)
        self.act = nn.ReLU()

        self.block = nn.Sequential(self.conv, self.bn, self.act)

    def forward(self, x):
        return self.block(x)
```

Sau khi có khối Conv, ta dùng nó để xây dựng discriminator và generator

```python
class Discriminator(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = Conv(1, 16, 3, 1, 1)
        self.conv2 = Conv(16, 32, 3, 1, 1)
        self.conv3 = Conv(32, 64, 3, 1, 1)
        self.pooling = nn.MaxPool2d(2, 2)

        self.fc1 = nn.Linear(7*7*64, 1)


    def forward(self, x: torch.Tensor):
        x = self.conv1(x)
        x = self.pooling(x)

        x = self.conv2(x)
        x = self.pooling(x)

        x = self.conv3(x)

        x = x.flatten(1)

        x = self.fc1(x)
        return x
```

```python
class Generator(nn.Module):
    def __init__(self, z_dim: int = 20):
        super().__init__()
        self.fc1 = nn.Linear(z_dim, 7*7*256)

        self.conv1 = Conv(256, 128, 3, 1, 1)
        self.conv2 = Conv(128, 64, 3, 1, 1)
        self.conv3 = nn.Conv2d(64, 1, 1, 1, bias=True)
        self.upsample = nn.UpsamplingBilinear2d(scale_factor=2)

    def forward(self, x: torch.Tensor):
        x = self.fc1(x)
        x = x.reshape(-1, 256, 7, 7)

        x = self.upsample(x)
        x = self.conv1(x)

        x = self.upsample(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = nn.Sigmoid()(x)

        return x
```

* **Step 4: Thiết lập các optimizers, hàm train, visualise**

Set up các thông số training 
```python
GEN_LR = 5e-5
DISC_LR = 5e-5
n_critic = 5

gen_optimizer = optim.Adam(generator.parameters(), lr=GEN_LR, amsgrad=True)
disc_optimizer = optim.Adam(disc.parameters(), lr=DISC_LR, amsgrad=True)
```

Hàm train 1 epoch

```python
def train_1_epoch(loader, n_critic: int = 5, c: float = 0.01):
    wasserstein_distance_1_epoch = 0
    for real_data, _ in tqdm(loader):
        real_data = real_data.to(DEVICE)
        z = torch.randn((BATCH_SIZE, Z_DIM)).to(DEVICE)
        generated_data = generator(z)
        for _ in range(n_critic):
            disc_optimizer.zero_grad()
            wasserstein_distance = -(torch.mean(disc(real_data)) - torch.mean(disc(generated_data)))
            
            wasserstein_distance.backward(retain_graph=True)
            disc_optimizer.step()

            for p in disc.parameters():
                p.data.clamp_(-c, c)
        wasserstein_distance_1_epoch -= wasserstein_distance.item()
        gen_loss = -torch.mean(disc(generated_data))
        gen_optimizer.zero_grad()
        gen_loss.backward()
        gen_optimizer.step()

    return wasserstein_distance_1_epoch
```

Hàm visualise 

```python
@torch.no_grad()
def visualise():
    generator.eval()
    z = torch.randn((16, Z_DIM)).to(DEVICE)
    generated_data = generator(z).cpu()

    fig, ax = plt.subplots(figsize=(10, 10))
    plt.axis("off")
    ax.imshow(np.transpose(vutils.make_grid(generated_data, padding=2, normalize=True), (1, 2, 0)))
    plt.show()
    generator.train()
```

```python
def train(epochs: int):
    for i in range(1, epochs+1):
        wasserstein_distance_1_epoch = train_1_epoch(train_loader)
        print(f"Epoch: {i}  Wasserstein distance of an epoch: {wasserstein_distance_1_epoch}")
        visualise()
```

* **Step 5: Training model**

```python
train(1000)
```





### 4. Giải thích toán học WGAN (optional)


### 5. Điểm yếu còn tồn đọng của WGAN

Như các bạn thấy ở trên, tác giải sử dụng phương pháp weight clipping để làm cho mạng discriminator behave giống hàm 1-Lipschitz nhất có thể. Tuy nhiên cách này không đúng và phụ thuộc khá nhiều vào hyperparameter _c_, tác giả cũng thừa nhận trong paper "_Weight clipping is a clearly terrible way to enforce a Lipschitz constraint_". Nếu muốn hệ số c lớn hơn, ta phải đánh đổi bằng việc sử dụng mạng discriminator lớn hơn và hội tụ chậm hơn, ngược lại, nếu c nhỏ hơn thì hội tụ sẽ nhanh hơn nhưng sẽ bị vanishing gradient. 

### 6. Kết luận 

